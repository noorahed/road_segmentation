{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47587a27",
   "metadata": {
    "id": "47587a27"
   },
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b2ec79",
   "metadata": {
    "id": "78b2ec79"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2 as cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Activation, Add, Conv2D, Multiply, MaxPooling2D, UpSampling2D, Dropout, concatenate, Lambda\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import keras.backend as K\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "import scipy.ndimage as ndimage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f4101",
   "metadata": {
    "id": "cb4f4101"
   },
   "source": [
    "## Metrics and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b624135",
   "metadata": {
    "id": "7b624135"
   },
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred):\n",
    "    # Round predictions to binary values (0 or 1)\n",
    "    y_pred = tf.round(y_pred)\n",
    "\n",
    "    # Calculate True Positives, False Positives, and False Negatives\n",
    "    true_positives = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    possible_positives = tf.reduce_sum(tf.cast(y_true, tf.float32))\n",
    "    predicted_positives = tf.reduce_sum(tf.cast(y_pred, tf.float32))\n",
    "\n",
    "    # Calculate Precision and Recall\n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "\n",
    "    return f1\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1_score\", **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.false_positives = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.false_negatives = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.round(y_pred)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32)))\n",
    "        self.false_positives.assign_add(tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32)))\n",
    "        self.false_negatives.assign_add(tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32)))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        return 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n",
    "\n",
    "\n",
    "def sobel_edge_detection(x):\n",
    "    sobel_x = tf.image.sobel_edges(x)\n",
    "    edges = tf.sqrt(sobel_x[..., 0] ** 2 + sobel_x[..., 1] ** 2)  # Combine the gradients in X and Y directions\n",
    "    return edges\n",
    "\n",
    "one_weight = 0.55\n",
    "zero_weight = 0.45\n",
    "def create_weighted_binary_cross_entropy(zero_weight, one_weight):\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "        # Calculate the binary crossentropy\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Apply the weights\n",
    "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        # Return the mean error\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy\n",
    "\n",
    "def attention_gate(g, s, num_filters):\n",
    "    Wg = L.Conv2D(num_filters, 1, padding=\"same\")(g)\n",
    "    Wg = L.BatchNormalization()(Wg)\n",
    "\n",
    "    Ws = L.Conv2D(num_filters, 1, padding=\"same\")(s)\n",
    "    Ws = L.BatchNormalization()(Ws)\n",
    "\n",
    "    out = L.Activation(\"relu\")(Wg + Ws)\n",
    "    out = L.Conv2D(num_filters, 1, padding=\"same\")(out)\n",
    "    out = L.Activation(\"sigmoid\")(out)\n",
    "\n",
    "    return out * s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19079744",
   "metadata": {},
   "source": [
    "## eagle loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a2499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afd2c3ba",
   "metadata": {
    "id": "afd2c3ba"
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9b5aba",
   "metadata": {
    "id": "0e9b5aba"
   },
   "outputs": [],
   "source": [
    "def fat_unet(input_size, verbose = False):\n",
    "    #Taken from https://github.com/zhixuhao/unet/blob/master/model.py\n",
    "    inputs = Input(input_size)\n",
    "    s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(s)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    drop3 = Dropout(0.3)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    attention6 = attention_gate(drop4, up6, 512)\n",
    "    merge6 = concatenate([attention6,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    attention7 = attention_gate(drop3, up7, 256)\n",
    "    merge7 = concatenate([attention7, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    attention8 = attention_gate(conv2, up8, 128)\n",
    "    merge8 = concatenate([attention8, up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    attention9 = attention_gate(conv1, up9, 64)\n",
    "    merge9 = concatenate([attention9, up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "\n",
    "    # Edge detection to enhance segmentation\n",
    "    edges = sobel_edge_detection(conv9)  # Apply edge detection to the last conv feature map\n",
    "\n",
    "    # Combine edge information with segmentation output\n",
    "    seg_output = Conv2D(1, 1, activation='sigmoid', name=\"segmentation_output\")(conv9)\n",
    "    \n",
    "    enhanced_seg_output = tf.keras.layers.Multiply()([seg_output, edges])  # Use edge features to refine segmentation\n",
    "\n",
    "    # Final model output\n",
    "    model = Model(inputs=inputs, outputs=enhanced_seg_output, name='fat_unet')\n",
    "    \n",
    "    loss = create_weighted_binary_cross_entropy(zero_weight, one_weight)\n",
    "    \n",
    "    model.load_weights(r'C:/Users/LENOVO/Personal Projects/deep project/checkpoints/experiment82/experiment82.weights.h5')  # Load weights into the model\n",
    "    \n",
    "    model.compile(optimizer = Adam(learning_rate = 1e-4), loss = loss, metrics = [F1Score()])\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9687fe7",
   "metadata": {
    "id": "c9687fe7"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75cb9a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e75cb9a9",
    "outputId": "7ac4658b-4818-4f7b-c744-3f853c2a0c2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 180.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 256, 256, 3)\n",
      "(65, 256, 256, 3) (65, 256, 256, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_dir = r\"C:\\Users\\LENOVO\\Personal Projects\\deep project\\dataset\\training\\training orig\\images\"\n",
    "mask_dir = r\"C:\\Users\\LENOVO\\Personal Projects\\deep project\\dataset\\training\\training orig\\groundtruth\"\n",
    "\n",
    "input_size = (256, 256, 3)\n",
    "\n",
    "# data loading function\n",
    "def load_data(image_dir, mask_dir, input_size):\n",
    "    images, masks = [], []\n",
    "\n",
    "    for image_name in tqdm(os.listdir(image_dir), desc=\"Processing Images\"):\n",
    "        # Load and preprocess image\n",
    "        img_path = os.path.join(image_dir, image_name)\n",
    "        img = load_img(img_path, target_size=input_size[:2])\n",
    "        img = img_to_array(img) / 255.0  # Normalize\n",
    "\n",
    "        # Load and preprocess mask\n",
    "        mask_path = os.path.join(mask_dir, image_name)  # Assuming masks have the same name\n",
    "        mask = load_img(mask_path, target_size=input_size[:2], color_mode='grayscale')\n",
    "        mask = img_to_array(mask) / 255.0  # Normalize\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "images, masks = load_data(image_dir, mask_dir, input_size)\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "# test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.35, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92dc419",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c92dc419",
    "outputId": "23aba6d1-05de-4be3-aa4e-7859b0dcdadd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 256, 256, 3) (520, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_rotations_90_180_270(images, masks):\n",
    "    rotated_images, rotated_masks = [], []\n",
    "\n",
    "    for img, mask in zip(images, masks):\n",
    "\n",
    "        # Generate and append the rotations\n",
    "        for k in [1, 2, 3]:  # 90°, 180°, and 270° rotations\n",
    "            rotated_images.append(np.rot90(img, k=k, axes=(0, 1)))\n",
    "            rotated_masks.append(np.rot90(mask, k=k, axes=(0, 1)))\n",
    "\n",
    "    return np.array(rotated_images), np.array(rotated_masks)\n",
    "\n",
    "# Function to apply augmentations (rotations in this case)\n",
    "def apply_augments_15(images, masks):\n",
    "    \"\"\"Applies affine rotation augmentations (same for images and masks).\"\"\"\n",
    "    augmenter = iaa.Sequential(iaa.Affine(rotate=(-15, 15)))  # Rotate within [-45°, 45°]\n",
    "\n",
    "    # Ensure deterministic augmentations\n",
    "    deterministic_aug = augmenter.to_deterministic()\n",
    "\n",
    "    # Apply to both images and masks\n",
    "    augmented_images = deterministic_aug(images=images)\n",
    "    augmented_masks = deterministic_aug(images=masks)  # Same augmentations as images\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_masks)\n",
    "\n",
    "def apply_augments_30(images, masks):\n",
    "    \"\"\"Applies affine rotation augmentations (same for images and masks).\"\"\"\n",
    "    augmenter = iaa.Sequential(iaa.Affine(rotate=(-30, 30)))  # Rotate within [-45°, 45°]\n",
    "\n",
    "    # Ensure deterministic augmentations\n",
    "    deterministic_aug = augmenter.to_deterministic()\n",
    "\n",
    "    # Apply to both images and masks\n",
    "    augmented_images = deterministic_aug(images=images)\n",
    "    augmented_masks = deterministic_aug(images=masks)  # Same augmentations as images\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_masks)\n",
    "\n",
    "def apply_augments_45(images, masks):\n",
    "    \"\"\"Applies affine rotation augmentations (same for images and masks).\"\"\"\n",
    "    augmenter = iaa.Sequential(iaa.Affine(rotate=(-45, 45)))  # Rotate within [-45°, 45°]\n",
    "\n",
    "    # Ensure deterministic augmentations\n",
    "    deterministic_aug = augmenter.to_deterministic()\n",
    "\n",
    "    # Apply to both images and masks\n",
    "    augmented_images = deterministic_aug(images=images)\n",
    "    augmented_masks = deterministic_aug(images=masks)  # Same augmentations as images\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_masks)\n",
    "\n",
    "def apply_augments_60(images, masks):\n",
    "    \"\"\"Applies affine rotation augmentations (same for images and masks).\"\"\"\n",
    "    augmenter = iaa.Sequential(iaa.Affine(rotate=(-60, 60)))  # Rotate within [-45°, 45°]\n",
    "\n",
    "    # Ensure deterministic augmentations\n",
    "    deterministic_aug = augmenter.to_deterministic()\n",
    "\n",
    "    # Apply to both images and masks\n",
    "    augmented_images = deterministic_aug(images=images)\n",
    "    augmented_masks = deterministic_aug(images=masks)  # Same augmentations as images\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_masks)\n",
    "\n",
    "# Apply augmentation to training data only\n",
    "augmented_x1, augmented_y1 = apply_augments_15(X_train, y_train)\n",
    "\n",
    "augmented_x2, augmented_y2 = apply_augments_30(X_train, y_train)\n",
    "\n",
    "augmented_x3, augmented_y3 = apply_augments_45(X_train, y_train)\n",
    "\n",
    "augmented_x4, augmented_y4 = apply_augments_60(X_train, y_train)\n",
    "\n",
    "rotated_images, rotated_masks = get_rotations_90_180_270(X_train, y_train)\n",
    "\n",
    "X_train = np.concatenate((X_train, rotated_images), axis=0)\n",
    "y_train = np.concatenate((y_train, rotated_masks), axis=0)\n",
    "X_train = np.concatenate([X_train, augmented_x1], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_y1], axis=0)\n",
    "X_train = np.concatenate([X_train, augmented_x2], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_y2], axis=0)\n",
    "X_train = np.concatenate([X_train, augmented_x3], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_y3], axis=0)\n",
    "X_train = np.concatenate([X_train, augmented_x4], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_y4], axis=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0671db21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0671db21",
    "outputId": "326e8115-5fd9-408d-bcce-415ec85ecef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# train split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec5d32",
   "metadata": {
    "id": "60ec5d32"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43caccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = r\"C:/Users/LENOVO/Personal Projects/deep project/checkpoints/\"\n",
    "\n",
    "def create_new_experiment_folder(base_path):\n",
    "\n",
    "    # Get the list of existing folders\n",
    "    existing_folders = [name for name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, name))]\n",
    "\n",
    "    # Filter folders that match the experiment pattern\n",
    "    experiment_numbers = []\n",
    "    for folder in existing_folders:\n",
    "        if folder.startswith(\"experiment\"):\n",
    "            try:\n",
    "                num = int(folder[len(\"experiment\"):])\n",
    "                experiment_numbers.append(num)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Determine the next experiment number\n",
    "    next_experiment_number = max(experiment_numbers, default=0) + 1\n",
    "\n",
    "    # Create the new experiment folder\n",
    "    directory_name = f\"experiment{next_experiment_number}\"\n",
    "    new_folder_name = f\"{directory_name}/experiment{next_experiment_number}.weights.h5\"\n",
    "    new_folder_path = os.path.join(base_path, new_folder_name)\n",
    "    try:\n",
    "        os.mkdir(os.path.join(base_path,directory_name))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return new_folder_path\n",
    "\n",
    "CHECKPOINT_PATH = create_new_experiment_folder(CHECKPOINT_PATH)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=7, monitor='val_loss'),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_PATH, save_weights_only=True, verbose=1, save_best_only=True),\n",
    "        reduce_lr\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1474e957",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1474e957",
    "outputId": "4c7019aa-f54b-424c-b1a9-f501b77dd241",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fat_unet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 256, 256, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 256, 64  1792        ['lambda[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 256, 256, 64  36928       ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 128, 128, 64  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 128, 128, 12  73856       ['max_pooling2d[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 128, 128, 12  147584      ['conv2d_2[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0          ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 64, 256)  295168      ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 64, 64, 256)  590080      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 512)  1180160     ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 512)  2359808     ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32, 32, 512)  0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 1024  4719616     ['max_pooling2d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 1024  9438208     ['conv2d_8[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 16, 16, 1024  0           ['conv2d_9[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 32, 32, 1024  0           ['dropout_2[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 512)  2097664     ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 512)  262656      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 512)  262656      ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 512)  2048       ['conv2d_11[0][0]']              \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 32, 32, 512)  0          ['batch_normalization[0][0]',    \n",
      " da)                                                              'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 512)  0           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 512)  262656      ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 512)  0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 32, 32, 512)  0           ['activation_1[0][0]',           \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 1024  0           ['tf.math.multiply[0][0]',       \n",
      "                                )                                 'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 512)  4719104     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 32, 32, 512)  2359808     ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 512)  0          ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64, 64, 256)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 64, 64, 256)  524544      ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 64, 64, 256)  65792       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 64, 64, 256)  65792       ['conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_17[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_18[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 64, 64, 256)  0          ['batch_normalization_2[0][0]',  \n",
      " mbda)                                                            'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 64, 256)  0           ['tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 64, 64, 256)  65792       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 64, 64, 256)  0           ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 64, 64, 256)  0          ['activation_3[0][0]',           \n",
      " )                                                                'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 512)  0           ['tf.math.multiply_1[0][0]',     \n",
      "                                                                  'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 64, 64, 256)  1179904     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 64, 64, 256)  590080      ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 25  0          ['conv2d_21[0][0]']              \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 128, 128, 12  131200      ['up_sampling2d_2[0][0]']        \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 128, 128, 12  16512       ['conv2d_3[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 128, 128, 12  16512       ['conv2d_22[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128, 128, 12  512        ['conv2d_23[0][0]']              \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128, 128, 12  512        ['conv2d_24[0][0]']              \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 128, 128, 12  0          ['batch_normalization_4[0][0]',  \n",
      " mbda)                          8)                                'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 128, 128, 12  0           ['tf.__operators__.add_2[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 128, 128, 12  16512       ['activation_4[0][0]']           \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 128, 128, 12  0           ['conv2d_25[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 128, 128, 12  0          ['activation_5[0][0]',           \n",
      " )                              8)                                'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 25  0           ['tf.math.multiply_2[0][0]',     \n",
      "                                6)                                'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 128, 128, 12  295040      ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 128, 128, 12  147584      ['conv2d_26[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 12  0          ['conv2d_27[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 256, 256, 64  32832       ['up_sampling2d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 256, 256, 64  4160        ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 256, 256, 64  4160        ['conv2d_28[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 256, 256, 64  256        ['conv2d_29[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 256, 256, 64  256        ['conv2d_30[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 256, 256, 64  0          ['batch_normalization_6[0][0]',  \n",
      " mbda)                          )                                 'batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 256, 256, 64  0           ['tf.__operators__.add_3[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 256, 256, 64  4160        ['activation_6[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 256, 256, 64  0           ['conv2d_31[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 256, 256, 64  0          ['activation_7[0][0]',           \n",
      " )                              )                                 'conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 256, 12  0           ['tf.math.multiply_3[0][0]',     \n",
      "                                8)                                'conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 256, 256, 64  73792       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 256, 256, 64  36928       ['conv2d_32[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 256, 256, 2)  1154        ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLambda  (4,)                0           ['conv2d_34[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  ()                  0           ['tf.compat.v1.shape[0][0]']     \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad (TFOpLambda)  (None, 258, 258, 2)  0           ['conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " tf.tile (TFOpLambda)           (3, 3, 2, 2)         0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.nn.depthwise_conv  (None, 256, 256, 4)  0          ['tf.compat.v1.pad[0][0]',       \n",
      " 2d (TFOpLambda)                                                  'tf.tile[0][0]']                \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (5,)                 0           ['tf.compat.v1.shape[0][0]']     \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 256, 256, 2,  0           ['tf.compat.v1.nn.depthwise_conv2\n",
      "                                 2)                              d[0][0]',                        \n",
      "                                                                  'tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 256, 256, 2)  0          ['tf.reshape[0][0]']             \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 256, 256, 2)  0          ['tf.reshape[0][0]']             \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.pow (TFOpLambda)       (None, 256, 256, 2)  0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.math.pow_1 (TFOpLambda)     (None, 256, 256, 2)  0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 256, 256, 2)  0          ['tf.math.pow[0][0]',            \n",
      " mbda)                                                            'tf.math.pow_1[0][0]']          \n",
      "                                                                                                  \n",
      " segmentation_output (Conv2D)   (None, 256, 256, 1)  3           ['conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " tf.math.sqrt (TFOpLambda)      (None, 256, 256, 2)  0           ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 256, 256, 2)  0           ['segmentation_output[0][0]',    \n",
      "                                                                  'tf.math.sqrt[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,087,877\n",
      "Trainable params: 32,084,037\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0943 - f1_score: 0.7914\n",
      "Epoch 1: val_loss improved from inf to 0.08303, saving model to C:/Users/LENOVO/Personal Projects/deep project/checkpoints/experiment170\\experiment170.weights.h5\n",
      "104/104 [==============================] - 40s 260ms/step - loss: 0.0943 - f1_score: 0.7914 - val_loss: 0.0830 - val_f1_score: 0.8137 - lr: 1.0000e-04\n",
      "Epoch 2/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0986 - f1_score: 0.7778\n",
      "Epoch 2: val_loss did not improve from 0.08303\n",
      "104/104 [==============================] - 26s 255ms/step - loss: 0.0986 - f1_score: 0.7778 - val_loss: 0.0979 - val_f1_score: 0.7777 - lr: 1.0000e-04\n",
      "Epoch 3/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0910 - f1_score: 0.8027\n",
      "Epoch 3: val_loss did not improve from 0.08303\n",
      "104/104 [==============================] - 26s 253ms/step - loss: 0.0910 - f1_score: 0.8027 - val_loss: 0.1088 - val_f1_score: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 4/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0893 - f1_score: 0.8040\n",
      "Epoch 4: val_loss did not improve from 0.08303\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 8.499999785271939e-05.\n",
      "104/104 [==============================] - 26s 253ms/step - loss: 0.0893 - f1_score: 0.8040 - val_loss: 0.0902 - val_f1_score: 0.8163 - lr: 1.0000e-04\n",
      "Epoch 5/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0847 - f1_score: 0.8140\n",
      "Epoch 5: val_loss improved from 0.08303 to 0.08135, saving model to C:/Users/LENOVO/Personal Projects/deep project/checkpoints/experiment170\\experiment170.weights.h5\n",
      "104/104 [==============================] - 27s 258ms/step - loss: 0.0847 - f1_score: 0.8140 - val_loss: 0.0813 - val_f1_score: 0.8216 - lr: 8.5000e-05\n",
      "Epoch 6/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0769 - f1_score: 0.8375\n",
      "Epoch 6: val_loss improved from 0.08135 to 0.07777, saving model to C:/Users/LENOVO/Personal Projects/deep project/checkpoints/experiment170\\experiment170.weights.h5\n",
      "104/104 [==============================] - 26s 252ms/step - loss: 0.0769 - f1_score: 0.8375 - val_loss: 0.0778 - val_f1_score: 0.8316 - lr: 8.5000e-05\n",
      "Epoch 7/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0733 - f1_score: 0.8447\n",
      "Epoch 7: val_loss improved from 0.07777 to 0.07617, saving model to C:/Users/LENOVO/Personal Projects/deep project/checkpoints/experiment170\\experiment170.weights.h5\n",
      "104/104 [==============================] - 27s 255ms/step - loss: 0.0733 - f1_score: 0.8447 - val_loss: 0.0762 - val_f1_score: 0.8358 - lr: 8.5000e-05\n",
      "Epoch 8/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0730 - f1_score: 0.8442\n",
      "Epoch 8: val_loss improved from 0.07617 to 0.07604, saving model to C:/Users/LENOVO/Personal Projects/deep project/checkpoints/experiment170\\experiment170.weights.h5\n",
      "104/104 [==============================] - 26s 253ms/step - loss: 0.0730 - f1_score: 0.8442 - val_loss: 0.0760 - val_f1_score: 0.8351 - lr: 8.5000e-05\n",
      "Epoch 9/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0793 - f1_score: 0.8226\n",
      "Epoch 9: val_loss did not improve from 0.07604\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.0793 - f1_score: 0.8226 - val_loss: 0.0845 - val_f1_score: 0.8183 - lr: 8.5000e-05\n",
      "Epoch 10/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0759 - f1_score: 0.8347\n",
      "Epoch 10: val_loss improved from 0.07604 to 0.07582, saving model to C:/Users/LENOVO/Personal Projects/deep project/checkpoints/experiment170\\experiment170.weights.h5\n",
      "104/104 [==============================] - 26s 253ms/step - loss: 0.0759 - f1_score: 0.8347 - val_loss: 0.0758 - val_f1_score: 0.8310 - lr: 8.5000e-05\n",
      "Epoch 11/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0693 - f1_score: 0.8518\n",
      "Epoch 11: val_loss did not improve from 0.07582\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.0693 - f1_score: 0.8518 - val_loss: 0.0828 - val_f1_score: 0.8269 - lr: 8.5000e-05\n",
      "Epoch 12/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0689 - f1_score: 0.8556\n",
      "Epoch 12: val_loss did not improve from 0.07582\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.0689 - f1_score: 0.8556 - val_loss: 0.0762 - val_f1_score: 0.8405 - lr: 8.5000e-05\n",
      "Epoch 13/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0658 - f1_score: 0.8616\n",
      "Epoch 13: val_loss improved from 0.07582 to 0.07455, saving model to C:/Users/LENOVO/Personal Projects/deep project/checkpoints/experiment170\\experiment170.weights.h5\n",
      "104/104 [==============================] - 26s 253ms/step - loss: 0.0658 - f1_score: 0.8616 - val_loss: 0.0745 - val_f1_score: 0.8432 - lr: 8.5000e-05\n",
      "Epoch 14/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0774 - f1_score: 0.8362\n",
      "Epoch 14: val_loss did not improve from 0.07455\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.0774 - f1_score: 0.8362 - val_loss: 0.0787 - val_f1_score: 0.8385 - lr: 8.5000e-05\n",
      "Epoch 15/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1033 - f1_score: 0.7635\n",
      "Epoch 15: val_loss did not improve from 0.07455\n",
      "104/104 [==============================] - 26s 250ms/step - loss: 0.1033 - f1_score: 0.7635 - val_loss: 0.1438 - val_f1_score: 0.6256 - lr: 8.5000e-05\n",
      "Epoch 16/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1255 - f1_score: 0.7007\n",
      "Epoch 16: val_loss did not improve from 0.07455\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 7.224999972095247e-05.\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.1255 - f1_score: 0.7007 - val_loss: 0.1313 - val_f1_score: 0.6882 - lr: 8.5000e-05\n",
      "Epoch 17/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.1080 - f1_score: 0.7494\n",
      "Epoch 17: val_loss did not improve from 0.07455\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.1080 - f1_score: 0.7494 - val_loss: 0.1003 - val_f1_score: 0.7738 - lr: 7.2250e-05\n",
      "Epoch 18/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0881 - f1_score: 0.8076\n",
      "Epoch 18: val_loss did not improve from 0.07455\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.0881 - f1_score: 0.8076 - val_loss: 0.0915 - val_f1_score: 0.7970 - lr: 7.2250e-05\n",
      "Epoch 19/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0837 - f1_score: 0.8173\n",
      "Epoch 19: val_loss did not improve from 0.07455\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.141249759821222e-05.\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.0837 - f1_score: 0.8173 - val_loss: 0.0859 - val_f1_score: 0.8094 - lr: 7.2250e-05\n",
      "Epoch 20/60\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.0733 - f1_score: 0.8422\n",
      "Epoch 20: val_loss did not improve from 0.07455\n",
      "104/104 [==============================] - 26s 251ms/step - loss: 0.0733 - f1_score: 0.8422 - val_loss: 0.0866 - val_f1_score: 0.8206 - lr: 6.1412e-05\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1:]\n",
    "\n",
    "# Create the model\n",
    "model = fat_unet(input_size=input_size, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,y_train,\n",
    "    batch_size= 4,\n",
    "    epochs=60,\n",
    "    validation_data=(X_val, y_val), \n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680a7e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "576403bb",
   "metadata": {
    "id": "576403bb"
   },
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f4afd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87f4afd8",
    "outputId": "0b16a766-72d0-48df-c369-884300c16b59"
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    test_loss, test_f1 = model.evaluate(X_test, y_test)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "\n",
    "    # Post-process predictions\n",
    "    # Threshold predictions for binary masks\n",
    "    y_pred_thresholded = (y_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Example: Visualize predictions alongside ground truth\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def display_predictions(X_test, y_test, y_pred, num_samples=5):\n",
    "        \"\"\"Displays test images, ground truth, and predictions side by side.\"\"\"\n",
    "        for i in range(num_samples):\n",
    "            plt.figure(figsize=(12, 4))\n",
    "\n",
    "            # Display the test image\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(X_test[i])\n",
    "            plt.title(\"Test Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Display the ground truth mask\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(y_test[i].squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Ground Truth\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Display the predicted mask\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(y_pred[i].squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3676e4",
   "metadata": {
    "id": "0a3676e4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_predictions(X_test, y_test, y_pred, num_samples=5):\n",
    "    \"\"\"\n",
    "    Display test samples, their ground truth, and model predictions.\n",
    "    \"\"\"\n",
    "    for i in range(num_samples):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Display the input image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(X_test[i].squeeze(), cmap=\"gray\")\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Display the ground truth mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(y_test[i].squeeze(), cmap=\"gray\")\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # If the prediction has multiple channels, display each channel\n",
    "        if y_pred[i].shape[-1] > 1:\n",
    "            num_channels = y_pred[i].shape[-1]\n",
    "            plt.figure(figsize=(15, 5))  # Adjust the figure size as needed\n",
    "            for channel in range(num_channels):\n",
    "                plt.subplot(1, num_channels, channel + 1)\n",
    "                plt.imshow(y_pred[i][..., channel], cmap=\"gray\")\n",
    "                plt.title(f\"Channel {channel}\")\n",
    "                plt.axis(\"off\")\n",
    "        else:\n",
    "            # Display the single channel prediction mask\n",
    "            pred_mask = y_pred[i].squeeze()\n",
    "            plt.imshow(pred_mask, cmap=\"gray\")\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    display_predictions(X_test, y_test, y_pred_thresholded, num_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575a8b5",
   "metadata": {
    "id": "c575a8b5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90c1fd",
   "metadata": {
    "id": "bc90c1fd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "with tf.device('/CPU:0'): \n",
    "    # Load test images using the modified function\n",
    "    def load_test_data(image_dir, input_size):\n",
    "        images = []\n",
    "        for folder_name in os.listdir(image_dir):\n",
    "            folder_path = os.path.join(image_dir, folder_name)\n",
    "\n",
    "            # Check if it's a directory\n",
    "            if os.path.isdir(folder_path):\n",
    "                for image_name in os.listdir(folder_path):\n",
    "                    img_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "                    # Only process if it's a .png file (or you can include other formats)\n",
    "                    if os.path.isfile(img_path) and image_name.endswith('.png'):\n",
    "                        # Load and preprocess image\n",
    "                        img = load_img(img_path, target_size=input_size[:2])  # Resize to match input size\n",
    "                        img = img_to_array(img)  # Convert to NumPy array\n",
    "                        img = img / 255.0  # Normalize to [0, 1]\n",
    "                        images.append(img)\n",
    "\n",
    "        return np.array(images)\n",
    "\n",
    "    # Path to test images directory\n",
    "    image_dir = r\"C:\\Users\\LENOVO\\Personal Projects\\deep project\\dataset\\test_set_images\\test_set_images\"\n",
    "\n",
    "    # Define the input size (same as training)\n",
    "    input_size = (256, 256, 3)\n",
    "\n",
    "    # Load test images\n",
    "    test_images = load_test_data(image_dir, input_size)\n",
    "    print(f\"Test images shape: {test_images.shape}\")  # Check the shape\n",
    "\n",
    "    # Run predictions\n",
    "    predictions = model.predict(test_images)\n",
    "\n",
    "    # Post-process the predictions (assuming binary segmentation)\n",
    "    # Convert the predicted probabilities to binary mask\n",
    "    predicted_masks = (predictions > 0.5).astype(np.uint8)  # Apply a 0.5 threshold\n",
    "\"\"\"\n",
    "    # Visualize the input images and their predicted masks\n",
    "    for i in range(len(test_images)):\n",
    "        original_image = test_images[i]\n",
    "        predicted_mask = predicted_masks[i]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Original image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f\"Original Image {i+1}\")\n",
    "        plt.imshow(original_image.squeeze())\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Use predictions for the predicted mask\n",
    "        predicted_mask = np.argmax(predictions[i], axis=-1)  # Apply to 'predictions', not 'y_pred'\n",
    "\n",
    "        # Handle multi-channel case (if applicable)\n",
    "        if predicted_mask.ndim == 3:  # Multiple channels\n",
    "            num_channels = predicted_mask.shape[-1]\n",
    "\n",
    "            # Create a grid to display all channels\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            for j in range(num_channels):\n",
    "                plt.subplot(1, num_channels, j + 1)\n",
    "                plt.imshow(predicted_mask[:, :, j].squeeze(), cmap=\"gray\")\n",
    "                plt.title(f\"Channel {j + 1}\")\n",
    "                plt.axis(\"off\")\n",
    "        else:  # Single-channel case\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(predicted_mask.squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9SUG-4c2HJFj",
   "metadata": {
    "id": "9SUG-4c2HJFj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the input images and their predicted masks\n",
    "for i in range(len(test_images)):\n",
    "    original_image = test_images[i]\n",
    "    predicted_mask = predicted_masks[i]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Original Image {i+1}\")\n",
    "    plt.imshow(original_image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    if predicted_mask.ndim == 3:  # Multiple channels\n",
    "        num_channels = predicted_mask.shape[-1]\n",
    "\n",
    "        # Create a grid to display all channels\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i in range(num_channels):\n",
    "            plt.subplot(1, num_channels, i + 1)\n",
    "            plt.imshow(predicted_mask[:, :, i], cmap=\"gray\")\n",
    "            plt.title(f\"Channel {i + 1}\")\n",
    "            plt.axis(\"off\")\n",
    "    else:  # Single-channel case\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(predicted_mask, cmap=\"gray\")\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dSDR6_d2VJ4i",
   "metadata": {
    "id": "dSDR6_d2VJ4i"
   },
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3cd30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'): \n",
    "\n",
    "    # Visualize the input images and their predicted masks\n",
    "    for i in range(len(test_images)):\n",
    "        original_image = test_images[i]\n",
    "        predicted_mask = predicted_masks[i]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Original image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f\"Original Image {i+1}\")\n",
    "        plt.imshow(original_image.squeeze())\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Use predictions for the predicted mask\n",
    "        #predicted_mask = np.argmax(predictions[i], axis=-1)  # Apply to 'predictions', not 'y_pred'\n",
    "\n",
    "        # Handle multi-channel case (if applicable)\n",
    "        if predicted_mask.ndim == 3:  # Multiple channels\n",
    "            num_channels = predicted_mask.shape[-1]\n",
    "\n",
    "            # Create a grid to display all channels\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            for j in range(num_channels):\n",
    "                plt.subplot(1, num_channels, j + 1)\n",
    "                plt.imshow(predicted_mask[:, :, j].squeeze(), cmap=\"gray\")\n",
    "                plt.title(f\"Channel {j + 1}\")\n",
    "                plt.axis(\"off\")\n",
    "        else:  # Single-channel case\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(predicted_mask.squeeze(), cmap=\"gray\")\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c18fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensor-gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
